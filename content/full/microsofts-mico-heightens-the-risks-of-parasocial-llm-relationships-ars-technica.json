{
  "title": "Microsoft’s Mico heightens the risks of parasocial LLM relationships - Ars Technica",
  "link": "https://arstechnica.com/ai/2025/10/microsofts-mico-heightens-the-risks-of-parasocial-llm-relationships/",
  "image": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/micoheart-1152x648-1761323845.png",
  "source": "Ars Technica",
  "category": "technology",
  "pubDate": "2025-10-24T17:07:57Z",
  "api": "NewsAPI",
  "full_text": "“It looks like you’re trying to find a friend. Would you like help?”\n\nMicrosoft is rolling out a new face for its AI, and its name is Mico. The company announced the new, animated blob-like avatar for Copilot’s voice mode yesterday as part of a “human-centered” rebranding of Microsoft’s Copilot AI efforts.\n\nMico is part of a Microsoft program dedicated to the idea that “technology should work in service of people,” Microsoft wrote. The company insists this effort is “not [about] chasing engagement or optimizing for screen time. We’re building AI that gets you back to your life. That deepens human connection.”\n\nMico has drawn instant and obvious comparisons to Clippy, the animated paperclip that popped up to offer help with Microsoft Office starting in the ’90s. Microsoft has leaned into this comparison with an Easter egg that can transform Mico into an animated Clippy.\n\n“Clippy walked so that we could run,” Microsoft AI Corporate VP Jacob Andreou joked in an interview with The Verge. “We all live in Clippy’s shadow in some sense.”\n\nBut while Clippy was an attempt to strengthen our connection to sterile Windows Help menus, Mico seems focused more on strengthening the parasocial relationships many people are already developing with LLMs. The defining interaction with Clippy was along the lines of “It looks like you’re writing a letter, would you like some help?” With Mico, the idea seems to be “It looks like you’re trying to find a friend. Would you like help?”\n\nThe term “parasocial relationship” was coined by academics in the ’50s to describe the feeling of intimacy that can develop between an audience and a media celebrity. Through repeated exposure, members of the audience can come to feel like they know the celebrity as a friend, even if the celebrity doesn’t know them at all.\n\nWhile mass media like radio, movies, and television can all feed into parasocial relationships, the Internet and smartphone revolutions have supercharged the opportunities we all have to feel like an online stranger is a close, personal confidante. From YouTube and podcast personalities to Instagram influencers or even your favorite blogger/journalist (hi), it’s easy to feel like you have a close connection with the people who create the content you see online every day.\n\nViewing all this content on a smartphone can flatten all these media and real-life personalities into a kind of undifferentiated media sludge. It can be all too easy to slot an audio message from your romantic partner into the same mental box as a stranger chatting about video games in a podcast. “When my phone does little mating calls of pings and buzzes, it could bring me updates from people I love, or show me alerts I never asked for from corporations hungry for my attention,” Julie Beck writes in an excellent Atlantic article about this phenomenon. “Picking my loved ones out of the never-ending stream of stuff on my phone requires extra effort.”\n\nThis is the world Mico seems to be trying to slide into, turning Copilot into another not-quite-real relationship mediated through your mobile device. But unlike the Instagram model who never seems to acknowledge your comments, Mico is always there to respond with a friendly smile and a warm, soothing voice.\n\nText-based AI interfaces are already frighteningly good at faking human personality in a way that encourages this kind of parasocial relationship, sometimes with disastrous results. But adding a friendly, Pixar-like face to Copilot’s voice mode may make it much easier to be sucked into feeling like Copilot isn’t just a neural network but a real, caring personality—one you might even start thinking of the same way you’d think of the real loved ones in your life.\n\nThat sounds less like technology focused on “deepen[ing] human connection” and more like the kind of technology that’s about “chasing engagement or optimizing for screen time.” After all, an AI that’s easier to talk to is an AI you’ll want to talk to more—and potentially pay more to access. If an AI chatbot with a warm, friendly face “earns your trust,” you’re a lot less likely to listen to the AI skeptics that generate what Microsoft calls “a lot of noise around AI.”\n\nIt’s unclear if Mico will end up being a beloved parasocial friend to millions of Copilot users or more of an ironically remembered annoyance like Clippy. But it won’t be the last attempt to put a cute, trustworthy face on large language models that don’t necessarily merit that level of trust. And we should all be wary of the parasocial psychology these efforts can feed into.\n\nArs Technica has been separating the signal from\n          the noise for over 25 years. With our unique combination of\n          technical savvy and wide-ranging interest in the technological arts\n          and sciences, Ars is the trusted source in a sea of information. After\n          all, you don’t need to know everything, only what’s important.",
  "images": [
    "https://cdn.arstechnica.net/wp-content/uploads/2025/10/micoheart-1152x648-1761323845.png",
    "https://cdn.arstechnica.net/wp-content/uploads/2025/10/GettyImages-2231949079.jpg",
    "https://cdn.arstechnica.net/wp-content/uploads/2016/05/k.orland-13.jpg",
    "https://cdn.arstechnica.net/civis/data/avatars/m/686/686083.jpg?1712863131",
    "https://cdn.arstechnica.net/civis/data/avatars/m/1075/1075491.jpg?1727953171"
  ]
}